# Talk 3: Data management

## Philosophy of organising your computational research

<br>

<span style="color: green; font-size: 1.5em;">
"Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why.”
</span>

- A Quick Guide to Organizing Computational Biology Projects - William Stafford Noble `r Citep(bib, "noble_quick_2009")`

## Overview

- Best practices for data storage & organisation
- Rules and conventions for naming files
- Typical components of genomics data analysis projects
- Records metadata and traceability
- Backups and archives
- Data transfers to and from remote computers

<!--
## Linux file system tree 

- In Linux, everything is a file organised in a tree structure
- A directory is just a file containing names of other files
- All directories branch off from the **root** directory

```{r}
#| fig-align: center
#| out-height: 400px
#| out-width: 700px
## Source: https://www.linuxfoundation.org/blog/blog/classic-sysadmin-the-linux-filesystem-explained
knitr::include_graphics("img/linux_file_tree.png")
```
-->

## Name files carefully

- In Linux, everything is a file organised in a tree structure
- Linux file names are case sensitive
  + Exclusively using lowercase makes remembering paths easier

- File names must be unique within their directory
  + Important especially for processes that overwrite by default

- Symbols allowed in filenames
  + Uppercase and lowercase letters
  + Digits
  + '-' (dash), '_' (underscore).

- Symbols to avoid in file names
  + e.g. % $ £ “ ‘ / \ | =
  + Do not put spaces into your file or directory names 
    ```
    # Interpret as two separate input arguments / directories
    $ ls My Directory
    ```
  + Use underscore or dash to separate words e.g. file_name.txt

## Name files carefully

- Use distinctive, human-readable names that give an indication of the content

- Follow a consistent pattern that is both user-friendly to read and machine-friendly to process 
  + e.g. sample1-replicate1-read1.fastq.gz

- Make use of suffixes to identify file formats
  + e.g. file.txt, file.fastq, file.bed

## Directory structure

- Put each project in its own directory, named after the project
- Organise files into directory structures that follow a consistent pattern

```
my_project/
    ├── data/
    │   ├── fastq/
    │   │   ├── sample1.fastq.gz
    │   │   └── sample2.fastq.gz
    │   └── annotations/
    │       ├── genome.gtf.gz
    │       └── sample_metadata.csv
    ├── code/
    │   ├── scripts/
    │   │   ├── hisat2.sh
    │   │   └── featurecounts.sh
    │   └── notebooks/
    │       ├── differential_expression.R
    │       └── pathway_analysis.R
    ├── results/
    │   ├── sample1.bam
    │   ├── sample2.bam
    │   └── read_counts.tsv
    └── README.txt
```

## Directory structure

- Sub-directories are commonly created for
  + Raw sequencing data (e.g. FASTQ files).
  + Publicly available data sets (e.g., Gene Expression Omnibus - NCBI)
  + Reference genome, index, and annotations (e.g. Ensembl FTP)
  + Analysis code (e.g. scripts, notebooks, pipelines)
  + Analysis output files (e.g. tables, plots, reports)

## Keeping records with the lab notebook

- A chronologically organised lab notebook
- Series of documents / text files where you record your analysis process
- Entries in the notebook should be dated, and verbose
- Record all the steps used to process data
- Include links to relevant websites, files, reports or plots
- Record your observations, conclusions, and ideas for future work
- Include your lab notebook in a git repository, and upload to GitHub regularly

## Backup your work

- A backup is a copy of important data that is stored at regular intervals of time in an alternative location, so that it can be recovered the original data is deleted or becomes corrupted
- A backup should be in a different computer preferably physically distant from the original source
- Ways
  + Many Linux clusters offer backup options 
    - Ask system administrator about options and cost
  + The University offers backup services for single-user and multi-user machines [Central Backup Service (HFS)](https://services.it.ox.ac.uk/Service/data-services/backup)
- Use Git to version control your code and documentation (TBD, day 3)
  + Push changes to GitHub regularly

## Archive data

- When a project has been completed you can archive the data
  + Archiving systems reduce storage costs, but will often have longer access times
- Before using third-party systems, check with funders if they allow data to be hosted externally
  + Amazon Glacier
- Published data can be stored in public database at no cost
  + European Nucleotide Archive / Short Read Archive
  + ArrayExpress / GEO
- Archived data should be carefully organised and annotated with comprehensive metadata for traceability and discoverability
  +  Metadata is data providing information about other data

<!-- - WIMM IT team offers the [WIMM Keep](https://gatekeeper.imm.ox.ac.uk/index.cgi/about) (SSO login required) -->

## Data storage good practices

- Backup
  + Experimental raw data - data collected directly from experiments and all raw files that cannot be (or hard to) regenerate 
  + Analysis code 
  + Environment specifications - version of programs used in scripts and notebooks should be recorded and backed up
- Separate different analyses into different subdirectories

## Data storage good practices

- Avoid storing large uncompressed text files 
  + If you need to decompress for processing, compress afterwards
- Delete large intermediate files
  + If you perform multiple BAM processing steps only keep the final BAM file
- Keep only one copy of large files (excluding backups)
  + Use symbolic links (`ln -s`) to keep data together without copying files
- Regularly check your disk usage and available disk / quota with `du`

# Accessing files to/from remote hosts and from other online databases

## Transferring files across systems on the command line using `scp`

- `scp` (secure copy) uses SSH protocol to copy files securely

  + `scp` \<options> \<source path> \<destination path>
  
- Typing on the local machine, we can transfer files from OBDS server to local machine:

```
$ scp <username>@obds:<source path> <destination path>
```

- `scp` vs. `rsync`
   + `scp` - Straightforward copying of files using SSH
   + `rsync` - Synchronises files and directories between a source and a destination 

## Transferring files using FileZilla

- The [FileZilla Client](https://filezilla-project.org/download.php?platform=osx) is a free solution for transferring files between computers via FTP/SFTP/FTPS protocols
  + Transfer files between a local computer (where the FileZilla Client is installed) and a remote computer

:::: {.columns}

::: {.column width="70%"}

```{r}
#| fig-align: center
#| out-height: 400px
#| out-width: 600px
## Source: CCB Doks
knitr::include_graphics("img/filezilla-drop-areas.png")
```

:::

::: {.column width="30%"}

- To connect to remote server, provide:
  + Host     - name of remote server
  + Username - remote server username
  + Password - remote server password
  + Port     - 22 (secure ftp / sftp)

- Or use ssh key

<span style="color:green;">DEMO</span>

:::

::::

## Downloading public sequencing data

- ArrayExpress (EBI)
- ENA – European Nucleotide archive (EBI)
- ENCODE - Encyclopedia of DNA Elements
- Ensembl is the European database for reference genome sequence and annotation data
  + DNA, RNA and protein sequence data
  + Gene & genome annotations
    + Coding and non-coding transcripts
    + Repeats, CpG islands etc.
- GEO – Gene expression Omnibus (NCBI)
- GTEx - Genotype-Tissue Expression
- HCA - Human Cell Atlas
- SRA – Short Read Archive (NCBI)
- Download published datasets
  + Raw sequencing data (fastq files)
  + Processed results (bed, count matrix)
  + Metadata (experiment details, sample processing, data analysis)
  
## Downloading remote files using the command line

- We can use the Linux command line tool `wget` to download remote files e.g. from public databases like Ensembl

```
$ wget ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz

# -P to set a directory prefix where all files and subdirectories will be saved
$ wget \
      -P ~/ccb_demo \               
       ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz
```

<!-- <span style="color:green;">DEMO with exercises [Ensembl](http://www.ensembl.org/index.html)</span> -->

## Downloading remote files using the command line

- `wget`
- `curl`
   + Like `wget`, `curl` can be used for downloading files from the Internet 
   + The two have differences in terms of other functionality e.g. `wget` can download recursively

## Verify integrity of downloads

- We can check if files have downloaded correctly using checksums
- A **checksum** is a value or code derived from digital data to verify its integrity
  + Like a unique fingerprint of a data
  
  ```{r}
  #| fig-align: center
  #| out-height: 70px
  #| out-width: 600px
  ## Source: CCB Doks
  knitr::include_graphics("img/md5_file.png")
  ```
  
- `md5sum`, `sha256sum`, `sum` are examples of commands to calculate and verify checksums

<!--   + They differ in algorithms used to calculate checksum -->

- Users can compare locally generated checksums with those provided by the source/databases to verify file integrity
  - Note that public databases differ in commands they use

<!-- <span style="color:green;">Illustrate</span> -->

## Verify integrity of downloads

- ENA uses `md5sum`

```
$ wget <http…/md5sum.txt>  # Download files along with reference checksum file to
                             your working machine
$ cat md5sum.txt           # View reference checksum file
```

```{r}
#| fig-align: center
#| out-height: 70px
#| out-width: 600px
## Source: CCB Doks
knitr::include_graphics("img/md5_file.png")
```

```
$ md5sum –c md5sum.txt     # Generate checksum for downloaded file/s and command also 
                             compares to value in reference checksum file
```

```{r}
#| fig-align: center
#| out-height: 70px
#| out-width: 600px
## Source: CCB Doks
knitr::include_graphics("img/md5sum-c.png")
```

- Ensembl uses `sum` 

<!-- <span style="color:green;">DEMO [Ensembl](http://www.ensembl.org/index.html)</span> -->

```
$ wget <http…/CHECKSUMS>   # Download files along with reference checksum file to
                             your working machine
$ cat CHECKSUMS            # View reference checksum file
$ sum <downloaded file>    # Generate checksum for downloaded file
                           # Then can compare by eye to value in reference checksum 
                             file
```

# Talk 3 Exercise 1

<!-- @sec-Talk3Exercise1 -->

## Talk 3 Exercise 1 – Working directory {#sec-Talk3Exercise1}

**Set up a working directory for RNAseq data analysis**

1. Create a new directory called **rnaseq** within **1_linux** in your course working directory. 

```{bash, eval = FALSE}
cd <course working dir>/1_linux/
mkdir rnaseq
```

2. Change into your **rnaseq/**. Create subdirectories called **fastq** and **genome**.

```{bash, eval = FALSE}
cd rnaseq/
mkdir fastq genome
```

3. Create symbolic links to a pair of fastq files in the rnaseq resources directory, in **fastq/**.

```{bash, eval = FALSE}
cd fastq/
ls <resources dir>/1_linux/rnaseq/
ln -s <resources dir>/1_linux/rnaseq/ERR1755082_1.fastq.gz ERR1755082_1.fastq.gz
ln -s <resources dir>/1_linux/rnaseq/ERR1755082_2.fastq.gz ERR1755082_2.fastq.gz
```

4. Copy the file **md5sum.txt** from rnaseq resources directory to **fastq**.

```{bash, eval = FALSE}
cp <resources dir>/1_linux/rnaseq/md5sum.txt md5sum.txt
```

5. Check that the checksums for your fastq files (downloaded from ENA) are correct.

```{bash, eval = FALSE}
md5sum -c --ignore-missing md5sum.txt
``` 

6. Download the fasta file for the mouse genome (GRCm38, release 102) from Ensembl to **genome/**.

```{bash, eval = FALSE}
# Go to Ensembl home, Downloads tab, FTP Download section then the table links 
# to most current release but you can work your way up to get to older releases
# This way is good because directory structure is similar across releases and 
# you should be able to find the file type you want in older releases

# Alternatively, you can change the text of the links below based on the release 
# you want (e.g. to download release 110, change 102 to 110 in the links) but 
# this approach is subject to typos and also you are not sure whether the link
# really (or still) exists or not

cd ../genome/
# Download Mus_musculus.GRCm38.dna.primary_assembly.fa.gz
wget \
https://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz
```

7. Download the annotation GTF file for the mouse (release 102) genome from Ensembl to **genome/**.

```{bash, eval = FALSE}
wget \
https://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz
```

## Talk 3 Exercise 1 – Bonus exercises

1. Check if genome downloaded correctly using the CHECKSUMS file that can also be downloaded from the database.

```{bash, eval = FALSE}
wget https://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/CHECKSUMS &
cat CHECKSUMS | grep 'Mus_musculus.GRCm38.dna.primary_assembly.fa.gz'
sum Mus_musculus.GRCm38.dna.primary_assembly.fa.gz
# The checksums value in CHECKSUMS file (downloaded from public database) for 
# 'Mus_musculus.GRCm38.dna.primary_assembly.fa.gz' should match the generated 
# checksums value (generated using sum command) for the file you downloaded
```

2. Check if GTF file downloaded correctly using the CHECKSUMS file that can also be downloaded from the database.

```{bash, eval = FALSE}
wget https://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/CHECKSUMS &
ls  # Because there is an existing CHECKSUMS for the genome, it downloaded GTF CHECKSUMS as CHECKSUMS.1
cat CHECKSUMS.1 | grep 'Mus_musculus.GRCm38.102.gtf.gz'
sum Mus_musculus.GRCm38.102.gtf.gz
```

3. Create a **README.txt** file in the **genome/** detailing how you obtained the public data files.

```{bash, eval = FALSE}
nano README.txt  # Or
touch README.txt
```

<!-- 9. Create a third directory called **3_analysis** in your **rnaseq** directory. Change into the new directory and create a subdirectory called **1_fastqc**. View rnaseq directory structure using `tree`. -->

<!-- ```{bash, eval = FALSE} -->
<!-- cd .. -->
<!-- mkdir 3_analysis -->
<!-- cd 3_analysis -->
<!-- mkdir 1_fastqc -->
<!-- cd .. -->
<!-- tree -->
<!-- ``` -->

4. Create an analysis lab-book called **rnaseq_commands.txt** in **rnaseq/**.

```{bash, eval = FALSE}
touch rnaseq_commands.txt
```

## References

```{r}
#| results: asis
PrintBibliography(bib)
```

# Bonus topics

## Advanced shell configuration 

- You may come across other Bash startup files other than ~/.bashrc e.g. ~/.profile, ~/.bash_profile

- Those other files perform a task similar to ~/.bashrc, but with subtle differences with respect to the order and the environment in which they are executed

- <span style="color:red;">We strongly discourage users from editing or creating those other files, stick with editing ~/.bashrc!</span>

## Advanced shell configuration - ~/.inputrc file

- This file allows you to customise how keystrokes are interpreted in the terminal emulator
- Like ~/.bashrc, it is a hidden file in the home directory that is applied on login
- Common shortcuts added to ~/.inputrc are:
  + In case of ambiguity when using tab completion, display options right away instead of asking whether to display them or not
  ```
  set show-all-if-ambiguous on
  ```
  
  + Search history of commands (forward or backward) starting with the pattern you typed (could be a command therefore)
  + "\<key combination>":\<command>
  
  ```
  "\e[A":history-search-backward
  "\e[B":history-search-forward
  ```
  
  + [List of key combinations and commands from Bash Reference Manual](http://www.gnu.org/software/bash/manual/bashref.html#Command-Line-Editing)

<span style="color:green;">DEMO on OBDS server</span>
