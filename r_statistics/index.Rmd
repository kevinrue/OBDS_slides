---
title: 'Statistics'
subtitle: '<i class="fab fa-r-project"></i>'
author: "Kevin Rue-Albrecht"
institute: "Oxford Biomedical Data Science Training Programme"
date: "2021-02-23 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, metropolis, rladies-fonts, "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
# uncomment this line to produce HTML and PDF in RStudio:
knit: pagedown::chrome_print
---

layout: true

<div class="my-header"><img src="img/ox_brand1_pos.gif" alt="Oxford University Logo" align="right" height="90%"></div>

<div class="my-footer"><span>
Kevin Rue-Albrecht
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
Statistics and Machine Learning in <i class="fab fa-r-project"></i>
</span></div>

```{r setup, include = FALSE}
stopifnot(requireNamespace("htmltools"))
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, error = FALSE,
  include = FALSE
)
stopifnot(require(tidyverse))
stopifnot(require(ALL))
stopifnot(require(hgu95av2.db))
stopifnot(require(matrixStats))
stopifnot(require(cowplot))
stopifnot(require(AnnotationDbi))
stopifnot(require(GO.db))
data(ALL)
```

```{r, load_refs, include=FALSE, cache=FALSE}
options(htmltools.dir.version = FALSE)
library(RefManageR)
BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  cite.style = "authoryear",
  max.names = 2,
  style = "markdown",
  hyperlink = "to.doc",
  dashed = TRUE)
bib <- ReadBib("bibliography.bib")
```

---

# Prerequisites

<br/>

.x-large-list[
- A clone of a personal GitHub repository for this course.

- A clone of a shared GitHub repository for this course.

- A working installation of [R](https://www.r-project.org/) (4.0.3).

- A working installation of [git](https://git-scm.com/).

- A working installation of [RStudio](https://rstudio.com/).
]

---

# Set up

- Create a new RStudio project, called `r_statistics`.
  Store the project within your personal <i class="fab fa-git"></i> repository.

- Download the shared folder `r/statistics/data` from the CGAT cluster.
  Store the folder within the new RStudio project.

The result on your computer should look like the following

```
  obds_training/
  |_ r_statistics/
    |_ data/
      |_ ... (data files)
```

- Launch the R project in `r_statistics`.

---

# Learning objectives

<br/>

.xx-large-list[
- Learn to use the builtin statistical distributions

- Learn to use the builtin statistical tests

- Run tests and interpret results 

- Visualise  data and test results 
]

---

# <i class="fab fa-r-project"></i> is built for statistics

- <i class="fab fa-r-project"></i> includes a number of common statistical distributions:

  + The Normal Distribution
  
  + The Binomial Distribution
  
  + The Poisson Distribution
  
  + ...

--

- <i class="fab fa-r-project"></i> implements a range of statistical tests:

  + Student's t-Test
  
  + Pearson's Chi-squared Test for Count Data
  
  + Wilcoxon Rank Sum and Signed Rank Tests
  
  + ...

---

# <i class="fab fa-r-project"></i> Functions for Probability Distributions

<!-- Sources
https://www.stat.umn.edu/geyer/old/5101/rlook.html
-->

.pull-left[
.xx-small-table[
|Distribution                   |Probability |Quantile    |Density     |Random      |
|:------------------------------|:-----------|:-----------|:-----------|:-----------|
|Beta                           |`pbeta`     |`qbeta`     |`dbeta`     |`rbeta`     |
|Binomial                       |`pbinom`    |`qbinom`    |`dbinom`    |`rbinom`    |
|Cauchy                         |`pcauchy`   |`qcauchy`   |`dcauchy`   |`rcauchy`   |
|Chi-Square                     |`pchisq`    |`qchisq`    |`dchisq`    |`rchisq`    |
|Exponential                    |`pexp`      |`qexp`      |`dexp`      |`rexp`      |
|F                              |`pf`        |`qf`        |`df`        |`rf`        |
|Gamma                          |`pgamma`    |`qgamma`    |`dgamma`    |`rgamma`    |
|Geometric                      |`pgeom`     |`qgeom`     |`dgeom`     |`rgeom`     |
|Hypergeometric                 |`phyper`    |`qhyper`    |`dhyper`    |`rhyper`    |
|Logistic                       |`plogis`    |`qlogis`    |`dlogis`    |`rlogis`    |
|Log Normal                     |`plnorm`    |`qlnorm`    |`dlnorm`    |`rlnorm`    |
|Negative Binomial              |`pnbinom`   |`qnbinom`   |`dnbinom`   |`rnbinom`   |
|Normal                         |`pnorm`     |`qnorm`     |`dnorm`     |`rnorm`     |
|Poisson                        |`ppois`     |`qpois`     |`dpois`     |`rpois`     |
|Student t                      |`pt`        |`qt`        |`dt`        |`rt`        |
|Studentized Range              |`ptukey`    |`qtukey`    |`dtukey`    |`rtukey`    |
|Uniform                        |`punif`     |`qunif`     |`dunif`     |`runif`     |
|Weibull                        |`pweibull`  |`qweibull`  |`dweibull`  |`rweibull`  |
|Wilcoxon Rank Sum Statistic    |`pwilcox`   |`qwilcox`   |`dwilcox`   |`rwilcox`   |
|Wilcoxon Signed Rank Statistic |`psignrank` |`qsignrank` |`dsignrank` |`rsignrank` |
]
]

.pull-right[
.small-text[
- Each distribution has a root name, e.g. `norm`

- Every distribution has four functions.

- The root name is prefixed by one of the letters:

  + `p` for "probability", the cumulative distribution function (c. d. f.)
  
  + `q` for "quantile", the inverse c. d. f.
  
  + `d` for "density", the density function (p. f. or p. d. f.)
  
  + `r` for "random", a random variable having the specified distribution
]
]

---

# The normal distribution

## Notation

$${\mathcal {N}}(\mu ,\sigma ^{2})$$
--

## Parameters

- ${\mu \in \mathbb {R} }$ = mean (location)

- ${ \sigma ^{2}>0}$ = variance (squared scale)

--

## Properties

.pull-left[
- Median: ${ \mu }$

- Mode: ${ \mu }$
]

.pull-right[
- Variance: ${ \sigma ^{2} }$
]

--

- Probability density function (PDF): ${\displaystyle {\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}}$

---

# The standard normal distribution

```{r}
slide_mean <- 0; slide_sd <- 1;
```

Standard normal distribution with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)")
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.")
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)")
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution")
```

```{r, include=TRUE, echo=FALSE, fig.width=12, fig.height=7}
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

---

# A parameterised normal distribution

```{r}
slide_mean <- 50; slide_sd <- 100;
```

Normal distribution parameterised with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)")
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.")
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantile, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)")
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution")
```

```{r, include=TRUE, echo=FALSE, fig.width=12, fig.height=7}
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

---

# A parameterised binomial distribution

```{r}
slide_size <- 50; slide_prob <- 0.1;
```

Binomial distribution parameterised with size `r slide_size` and probability `r slide_prob`.
This distribution models an experiment where a coin is tossed 50 times, and the probability of observing head is 10%.

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  pbinom = pbinom(q = quantile, size = slide_size, prob = slide_prob)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pbinom)) +
  labs(
    title = sprintf("pbinom(q = quantile, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the cumulative distribution function (c. d. f.)")
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qbinom = qbinom(p = quantile, size = slide_size, prob = slide_prob)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qbinom)) +
  labs(
    title = sprintf("qbinom(p = quantile, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the inverse c. d. f.")
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  dbinom = dbinom(x = quantile, size = slide_size, prob = slide_prob)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dbinom)) +
  labs(
    title = sprintf("dbinom(x = quantile, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the density function (p. f. or p. d. f.)")
```

```{r}
x <- tibble(
  rbinom = rbinom(n = 1E3, size = slide_size, prob = slide_prob)
)
gg4 <- ggplot(x, aes(rbinom)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  labs(
    title = sprintf("rbinom(n = 1E3, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "a random variable having the specified distribution")
```

```{r, include=TRUE, echo=FALSE, fig.width=12, fig.height=7}
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

---

# Exercise

## Generate and visualise a distribution

- Generate a vector of 1000 normally distributed values with mean 10 and standard deviation 5.

- Inspect the output of the `summary()` function for that vector.

- Compute the mean and standard deviation for those values.

- Compute the deciles (i.e. 10 evenly spaced quantiles) for those values.

- Visualise the distribution of those values as a histogram.

- Visualise as vertical lines on the histogram: the mean (red solid), median (red dashed), one standard deviation from the mean (blue solid), and one median absolute deviation from the median (blue dashed).

- Generate a new vector with _a lot_ more values (e.g., one million).
  Draw again a histogram.
  How does the distribution compare with more data points?

---

# Exercise

## Query distributions and probabilities

For the standard normal distribution ${\mathcal {N}}(\mu=0 ,\sigma ^{2}=1)$:

- Plot the cumulative distribution function in the range $[-5, 5]$.

- Plot the inverse cumulative distribution function for quantiles in 0.01 increment.

- Plot the density function in the range $[-5, 5]$.

- What is the probability of observing a value greater than 2?

- What is the probability of observing a value between -2 and 2?

- What is the probability of observing a value more extreme than -2 or 2?

---

# Exercise

## Compute an Empirical Cumulative Distribution Function

- Use the `ecdf()` function to compute the empirical cumulative distribution function for the variable `Sepal.Length` in the `iris` data set.

- Use the `plot()` function to visualise the empirical cumulative distribution function.

- Use the `knots()` function on the `ecdf` output and compare this with the list of unique values for the variable `Sepal.Length`.

---

# <i class="fab fa-r-project"></i> Functions for Statistical Testing

In the `r BiocStyle::CRANpkg("stats")` package, functions that implement statistical tests have a named that ends in `.test`.

```{r, include=TRUE, echo=FALSE, results='asis'}
cat(paste(sprintf("`%s`", sort(grep("\\.test$", ls("package:stats"), value=TRUE))), collapse = ", "))
```

Each of those functions comes with a help page with programmatic usage, statistical advice, and external references to published work.

```{r, include=TRUE}
?pairwise.t.test
help(pairwise.t.test)
```

---

# The five steps of hypothesis testing

## General principles of hypothesis testing

1. Decide on the effect that you are interested in, **design** a suitable experiment or study, pick a data summary function and test statistic.

2. Set up a **null hypothesis**, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.

3. Decide on the **rejection region**, i.e., a subset of possible outcomes whose total probability is small.

4. Do the experiment and collect the data; compute the **test statistic**.

5. Make a **decision**: reject the null hypothesis if the test statistic is in the rejection region.

---

# Parametric tests and Non-parametric equivalents

```{r}
tibble(
  "Parametric test" = c(
    "Paired t-test",
    "Unpaired t-test",
    "Pearson correlation",
    "One-way Analysis of Variance"),
  "Non-parametric equivalent" = c(
    "Wilcoxon Rank sum test",
    "Mann-Whitney U test",
    "Spearman correlation",
    "Kruskal–Wallis test")
) %>% knitr::kable()
```

When parametric assumptions are not met, non-parametric tests equivalent should be used.

.large-table[
|Parametric test              |Non-parametric equivalent |
|:----------------------------|:-------------------------|
|Paired t-test                |Wilcoxon Rank sum test    |
|Unpaired t-test              |Mann-Whitney U test       |
|Pearson correlation          |Spearman correlation      |
|One-way Analysis of Variance |Kruskal–Wallis test       |
]

--

Non-parametric tests make fewer assumptions, as such:

- they have wider applicability.
- they may be applied in situations where less is known about the data.
- they are more robust.
- ..., however, fewer assumption gives non-parametric tests _less_ power than their parametric equivalent.

???

**Credits:** [https://www.healthknowledge.org.uk/](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/parametric-nonparametric-tests)

---

# Parametric t-test

.pull-left[
## Two normal distributions

```{r, include=TRUE}
set.seed(10)
x <- rnorm(n = 100, mean = 0, sd = 1)
y <- rnorm(n = 100, mean = 1, sd = 1)
```

```{r, include=TRUE, echo=FALSE, fig.height=5}
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
ggplot(test_data, aes(value)) +
  geom_histogram(fill = "grey", color = "black") +
  facet_wrap(~group, ncol = 1) +
  theme_minimal()
```
]

--

.pull-right[
## Unpaired t-test

.x-small-code[
```{r, include=TRUE}
t.test(value ~ group, test_data)
```

Compare with

```{r, include=TRUE, eval=FALSE}
t.test(x, y)
t.test(y, x)
```
]
]

---

# Non-parametric wilcoxon test

.pull-left[
## Two uniform distributions

```{r, include=TRUE}
set.seed(10)
x <- runif(n = 100, min = 1, max = 11)
y <- runif(n = 100, min = 3, max = 13)
```

```{r, include=TRUE, echo=FALSE, fig.height=5}
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  theme_minimal()
gg
```
]

--

.pull-right[
## Mann-Whitney U test

.x-small-code[
```{r, include=TRUE}
wilcox.test(value ~ group, test_data)
```
]

## Directed hypothesis

.x-small-code[
```{r, include=TRUE}
wilcox.test(value ~ group, test_data, alternative = "less")
```
]
]

---

# Paired test

For each sample, the two measurements are related to one another; e.g. patients measured before and after a treatment.

.pull-left[
.small-code[
```{r, include=TRUE}
set.seed(10)
n_sample <- 100
x <- runif(n = n_sample, min = 10, max = 20)
y <- x + 2 + rnorm(n = n_sample, mean = 0, sd = 1)
```
]

```{r}
test_data <- tibble(
  sample = paste("sample", seq_len(n_sample)),
  x = x,
  y = y
) %>% pivot_longer(cols = c(x, y), names_to = "variable")
```

```{r, include=TRUE, echo=FALSE, fig.height=5}
ggplot(test_data, aes(variable, value)) +
  geom_line(aes(group = sample), size = 0.1) +
  geom_point() +
  theme_minimal()
```
]

--

.pull-right[
.small-code[
```{r, include=TRUE}
t.test(value ~ variable, test_data, paired = TRUE)
```
]

**Note:**
What is actually tested is whether the mean of the differences between the paired $x$ and $y$ measurements is different from 0.
]

---

# Analysis of Variance (ANOVA)

Is the variance _between_ groups larger than the variance _within_ groups?

```{r, include=TRUE, echo=FALSE, fig.align='center'}
knitr::include_graphics("https://www.scielo.br/img/revistas/rbccv/v33n6//0102-7638-rbccv-33-06-000V-gf04.jpg")
```

`r Citet(bib, "liguori2018data")`

---

# Analysis of Variance (ANOVA)

.pull-left[
.small-code[
```{r, include=TRUE}
set.seed(10)
n_sample <- 1000
x1 <- rnorm(n = n_sample, mean = 10, sd = 2)
x2 <- x1 + 5 + rnorm(n = n_sample, mean = 0, sd = 1)
x3 <- x2 + 0 + rnorm(n = n_sample, mean = 0, sd = 0.5)
test_data <- bind_rows(
  tibble(group = "x1", value = x1),
  tibble(group = "x2", value = x2),
  tibble(group = "x3", value = x3)
)
```

```{r, include=TRUE, echo=FALSE, fig.height=5}
test_data <- bind_rows(
  tibble(group = "x1", value = x1),
  tibble(group = "x2", value = x2),
  tibble(group = "x3", value = x3)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black")
gg
```
]
]

.pull-right[
.small-code[
```{r, include=TRUE}
(out <- aov(value ~ group, test_data))
summary(out)
```
]
]

---

# Linear models

Describe a continuous response variable as a function of one or more predictor variables.

.pull-left[
```{r}
set.seed(10)
test_data <- tibble(
  x = rnorm(n = 50, mean = 0, sd = 1),
  y = 10 + 2.5 * x + rnorm(n = 50, mean = 0, sd = 0.5))
```

```{r, include=TRUE, echo=FALSE, fig.height=4}
ggplot(test_data, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "glm", se = FALSE)
```

- What is the slope?
- What is the intercept?
]

--

.pull-right[
```{r, include=TRUE}
lm(y ~ x, test_data)
```
]

---

# Linear models - Summary

.small-code[
```{r, include=TRUE}
lm(y ~ x, test_data) %>% summary()
```
]

---

# Fisher's Exact Test

- Test of independence between two categorical variables

- Alternative to the Chi-square test when the sample is not large enough.

  + Rule of thumb: when any of the _expected_ values in the contingency table is less than 5.

  + e.g., Gene set over-representation analysis (ORA)

.pull-left[
```{r, include=TRUE, echo=FALSE}
knitr::include_graphics("https://www.pathwaycommons.org/guide/primers/statistics/fishers_exact_test/table_1.png")
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE}
knitr::include_graphics("https://seqqc.files.wordpress.com/2019/07/screenshot-2019-07-25-at-14.30.54.png?w=300&h=253")
```
]

.footnote[
Further reading: [Towards data science](https://towardsdatascience.com/fishers-exact-test-in-r-independence-test-for-a-small-sample-56965db48e87)
]

---

# Fisher's Exact Test

|         | Men| Women| Total|
|:--------|---:|-----:|-----:|
|Studying |   1|     9|    10|
|Slacking |  11|     3|    14|
|Total    |  12|    12|    24|

Knowing that 10 of these 24 teenagers are studying, and that 12 of the 24 are female, and assuming the null hypothesis that men and women are equally likely to study, what is the probability that these 10 teenagers who are studying would be so unevenly distributed between the women and the men?

```{r, include=TRUE, echo=FALSE, out.height="100px", fig.align='center'}
knitr::include_graphics("https://wikimedia.org/api/rest_v1/media/math/render/svg/355286b9e9fdc48395ff99ab24694092ee8f5b49")
```

<br/>

```{r, include=TRUE, echo=FALSE, out.height="50px", fig.align='center'}
knitr::include_graphics("https://wikimedia.org/api/rest_v1/media/math/render/svg/dd800d46585be53665dfbb75291aeee58a279cd9")
```

---

# Beware of interpreting inadequate tests!

.pull-left[
## Two uniform distributions

```{r, include=TRUE}
set.seed(10)
n_size <- 10E3
x <- runif(n = n_size, min = 1, max = 11)
y <- runif(n = n_size, min = 3, max = 13)
```

```{r, include=TRUE, echo=FALSE, fig.height=5}
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black")
gg
```
]

.pull-right[
## Parametric (unpaired) t-test

.x-small-code[
```{r, include=TRUE}
t.test(value ~ group, test_data)
```
]
]

---

# Choosing a test

```{r, include=TRUE, echo=FALSE, out.height='400px', fig.align='center'}
knitr::include_graphics("https://www.scielo.br/img/revistas/rbccv/v33n6//0102-7638-rbccv-33-06-000V-gf01.jpg")
```

`r Citet(bib, "liguori2018data")`

See also <https://stats.idre.ucla.edu/other/mult-pkg/whatstat/>

---

# Knowledge assumptions - Central tendency

Tests make assumptions that must be met to for the results to be interpreted properly and with validity.

For instance, Student's t-Test expects values to be located around a central or typical value.

.pull-left[
```{r, echo=FALSE, include=TRUE, fig.height=5}
x_mean <- 0
x_sd <- 20
data_table <- tibble(x = as.integer(rnorm(n = 10E3, mean = x_mean, sd = x_sd)))
summary_table <- bind_rows(
  tibble(Value = "mean", value = mean(data_table$x)),
  tibble(Value = "median", value = median(data_table$x)),
  tibble(Value = "mode", value = as.integer(names(which.max(table(data_table$x)))))
)
data_table %>% 
  ggplot() +
  geom_histogram(aes(x = x), color = "black", fill = "grey") +
  geom_vline(aes(xintercept = value, color = Value), summary_table, size = 2, alpha = 0.3)
```
]

.pull-right[
Measures of central tendency include:

.large-list[
- the arithmetic mean
- the median
- the mode<sup>1</sup>
]
]

.footnote[
1. R does not have a standard in-built function to calculate the mode of a data series.
  The `mode()` function exists but has a completely unrelated purpose; it allows users to get or set the type or _storage mode_ of an object.
]

---

# Knowledge assumptions - Normality

In addition, Student's t-Test also expects values to be normally distributed.

.pull-left[
## Normal distribution

.small-code[
```{r include=TRUE}
x <- rnorm(n = 5000, mean = 0, sd = 1)
```

```{r, include=TRUE, echo=FALSE, fig.height=3}
ggplot(tibble(x=x)) +
  geom_histogram(aes(x), fill = "grey", color = "black", bins = 20) +
  theme_cowplot()
```

```{r, include=TRUE, fig.height=5}
shapiro.test(x)
```
]
]

.pull-right[
## Log-normal distribution

.small-code[
```{r, include=TRUE}
x <- 2^rnorm(n = 5000, mean = 0, sd = 1)
```

```{r, include=TRUE, echo=FALSE, fig.height=3}
ggplot(tibble(x=x)) +
  geom_histogram(aes(x), fill = "grey", color = "black", bins = 20) +
  theme_cowplot()
```

```{r, include=TRUE, fig.height=5}
shapiro.test(x)
```
]
]

---

# Knowledge assumptions - Normality

The Quantile-Quantile Plots (QQ plot) contrasts the quantiles of the observed distribution to those of a theoretical distribution.

.pull-left[
## Normal distribution

.small-code[
```{r, include=TRUE, echo=TRUE, fig.height=5}
x <- rnorm(n = 5000, mean = 5, sd = 3)
qqnorm(x)
```
]
]

.pull-right[
## Log-normal distribution

.small-code[
```{r, include=TRUE, echo=TRUE, fig.height=5}
x <- 2^rnorm(n = 5000, mean = 0, sd = 1)
qqnorm(x)
```
]
]

---

# Exercise

## Statistical tests

The `iris` data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris.

- Use the `summary()` function to view some information about each column.

- Visualise the distribution of `Sepal.Length`, stratified by species.

- Is `Sepal.Length` length normally distributed? Overall? Within each species?

- Is there a significant variation of `Sepal.Length` between the various species?

---

# Multiple-testing correction

## Hypothesis

"Jelly beans cause acne."

## Results

.pull-left[
- No link between jelly beans and acne.
]

.pull-right[
- No link between _brown_ jelly beans and acne.
- No link between _pink_ jelly beans and acne.
- ...
- Link between _green_ jelly beans and acne.
]

## News

> Green jelly beans linked to acne!
  95% confidence!
  Only 5% chance of coincidence!

.footnote[
[https://xkcd.com/882/](https://imgs.xkcd.com/comics/significant.png)
]

---

# Multiple-testing correction

.pull-left[
Distribution of $p$-values in an RNA-seq differential expression experiment

- True positive
- True negative
- False positive (type I error)
- False negative (type I error)

```{r, include=TRUE, echo=FALSE, fig.align='center', out.height='300px'}
knitr::include_graphics("https://www.huber.embl.de/msmb/figure/chap14-mt-awpvhist-1.png")
```
]

.pull-right[
.center[
Bonferroni correction
]

```{r, include=TRUE, echo=FALSE, fig.align='center', out.height='200px'}
knitr::include_graphics("https://www.huber.embl.de/msmb/figure/chap14-mt-bonferroni-1.png")
```

.center[
Benjamini-Hochberg procedure
]

```{r, include=TRUE, echo=FALSE, fig.align='center', out.height='200px'}
knitr::include_graphics("https://www.huber.embl.de/msmb/figure/chap14-mt-BH-1.png")
```
]

???

BH: Assuming no effect whatsoever, the p-values are expected to be uniformly distributed between 0 and 1.
That is, if you rank p-values in increasing order, you should see them increase uniformly from 0 to 1.
But if there is an effect, the associated p-values will be lower than expected by chance.
This is what the BH procedure is about:

- First, order the p-values in increasing order
- For a choice of FDR, find the largest p-value that is less than expected by chance
- Reject hypotheses until that p-value

---

# Multiple-testing correction

```{r}
set.seed(10)
n_tests <- 1000
compute_p_value <- function(dummy) {
  x <- rnorm(n = 100, mean = 0, sd = 1)
  y <- rnorm(n = 100, mean = 0, sd = 1)
  out <- t.test(x, y)
  out$p.value
}
result_table <- tibble(
  pvalue = vapply(X = seq_len(n_tests), FUN = compute_p_value, FUN.VALUE = numeric(1)),
  BH = p.adjust(p = pvalue, method = "BH"),
  bonferroni = p.adjust(p = pvalue, method = "bonferroni")
)
```

.pull-left[
Let us carry `r n_tests` tests between two normal distributions of mean 0 and standard deviation 1.

```{r, include=TRUE, echo=FALSE, fig.height=3}
data_table <- tibble(
  x = rnorm(n = 100, mean = 0, sd = 1),
  y = rnorm(n = 100, mean = 0, sd = 1)
) %>% pivot_longer(cols = c(x, y))
ggplot(data_table) +
  geom_boxplot(aes(name, value)) +
  geom_jitter(aes(name, value), width = 0.1)
```

```{r, include=TRUE, echo=FALSE, fig.height=3}
ggplot(result_table) +
  geom_histogram(aes(pvalue), fill = "grey", color = "black", breaks = seq(0, 1, 0.05)) +
  scale_x_continuous(limits = c(0, 1)) +
  labs(title = "Raw p-value")
```

```{r, include=TRUE, echo=FALSE, results='asis'}
cat(sprintf("There are %i raw p-values smaller than 0.05", sum(result_table$pvalue < 0.05), n_tests))
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.height=3}
ggplot(result_table) +
  geom_histogram(aes(BH), fill = "grey", color = "black", bins = 20) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "BH correction")
```

```{r, include=TRUE, echo=FALSE, results='asis'}
cat(sprintf("There are %i BH-corrected p-values smaller than 0.05", sum(result_table$BH < 0.05)))
```

```{r, include=TRUE, echo=FALSE, fig.height=3}
ggplot(result_table) +
  geom_histogram(aes(bonferroni), fill = "grey", color = "black", bins = 20) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "bonferroni correction")
```

```{r, include=TRUE, echo=FALSE, results='asis'}
cat(sprintf("There are %i bonferonni corrected p-values smaller than 0.05", sum(result_table$bonferroni < 0.05)))
```
]

---

# Exercise

## Testing & Multiple testing correction

- For each gene (i.e. row) in `logcounts.csv`, use `cell_metadata.csv` and a statistical test of your choice to identify gene differentially expressed in cells infected with _Salmonella_ relative to the control uninfected cells.

> **Suggestion:** write the code to test one gene, refactor the code into a function that returns the p-value, and use `vapply` to apply that function to all the genes.

- Visualise a bar plot of the p-values.

- Correct p-values for multiple testing.
  How many genes remain before and after multiple testing?

- Use `gene_metadata.csv` to get the gene name for the gene identifier associated with the smallest p-value.

---

# Exercise

## Over-representation analysis (ORA)

- For each gene ontology (GO) category in `human_go_bp.csv`, use the list of differentially expressed gene identifiers that you obtained in the previous exercise to find GO categories enriched for differentially expressed genes.

> **Suggestion:** write the code to test one gene set, refactor the code into a function that returns the p-value, and use `vapply` to apply that function to all the gene sets.

- Correct p-values for multiple testing.
  How many GO gene sets remain before and after multiple testing?

- Use `go_info.csv` to get the description for the GO identifier associated with the smallest p-value.

---

# Further reading

- [UCLouvain Bioinformatics Summer School 2019](https://uclouvain-cbio.github.io/BSS2019/)

  + [Introduction to Statistics and Machine Learning](https://github.com/ococrook/2019-BSS/raw/master/Intro2statsml.pdf) by Oliver M. Crook
  
  + [Practical: stats/ML](https://htmlpreview.github.io/?https://github.com/ococrook/2019-BSS/blob/master/practical/Intro2statmlPractical.html)

- [CSAMA](https://github.com/Bioconductor/CSAMA/tree/2019/lecture/1-monday) by the European Molecular Biology Laboratory (EMBL).

- [Statistic with R and dplyr and ggplot](https://www.youtube.com/watch?v=ANMuuq502rE) by Greg Martin

- [Susan Holmes](http://statweb.stanford.edu/~susan/) - [Introduction to Statistics for Biology and Biostatistics](http://statweb.stanford.edu/~susan/courses/s141/)

- Susan Holmes & Wolfgang Huber - [Modern Statistics for Modern Biology: Testing](https://www.huber.embl.de/msmb/Chap-Testing.html)

- [Bioconductor Case Studies](https://www.bioconductor.org/help/publications/books/bioconductor-case-studies/)

- [Introduction to Econometrics with R](https://www.econometrics-with-r.org/6-6-exercises-4.html)

---

# References

.small-text[
```{r, include=TRUE, echo=FALSE, results="asis"}
if (length(bib) > 0) {
  PrintBibliography(bib)
} else {
  cat("<p align='center'>n/a</p>")
}
```
]
